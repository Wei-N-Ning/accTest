War Story: Testing pipeline IO logic using Kenobi Driver
--------------------------------------------------------

#### the problem ####

While testing motion rnd's software libraries, one big challenge a developer has to face is that most pipeline IO logic
requires extremely complex inputs. Integration tests written for such logic needs to carefully prepare the input data
and pass them to the correct receivers.

Here is an excerpt from one of the publisher class:

```python
    def build(self, **kwds):

        # ...... skip 100+ lines

        self.serializedId = kwds.pop('serializedId', None)  # the atlas serializedIDPath for the elementPackage
        self.context = kwds.pop('context', None)

        self.bakeAtlasItemProxy = kwds.pop('bakeAtlasItemProxy', False)
        self.atlasItem = kwds.pop('atlasItem', None)  # atlas creature item
        self.elementsPackageHandler = kwds.pop('elementsPackageHandler', None)
        self.elementPackage = kwds.pop('elementPackage', None)  # atlas element package
        self.assetPackageHyref = kwds.pop('assetPackageHyref', None)  # assetPackage if known
        self.assetPackageSelection = kwds.pop('assetPackageSelection', None)
        self.elementName = kwds.pop('elementName', None)
        self.productElement = kwds.pop('productElement', None)
        self.creatureName = kwds.pop('creatureName', None)
        self.workspace = kwds.pop('workspace', None)  # atlas workspace
        self.project = kwds.pop('project', None)  # project
        self.envGroup = kwds.pop('envGroup', None

        # ..... skip the rest of the content
```

It's difficult to figure out what arguments are required by the build() method, since all of them are optional on the
interface. It is even harder to figure out the exact types of the input data.

#### test via UI ####

The good thing is that usually such data is generated by the UI logic which has done some kind of validation and
preparation, which means as long as the pipeline IO logic is triggered (directly or indirectly) by the highest UI layer
in the application, it is more or less guaranteed that the data passed into such method would be sufficient.
Therefore one would assume that, every pipeline IO acceptance test must go through the UI. Well, not necessarily.

#### find the boundary ####

It turns out that, in the case of publishing motion assets, the UI logic only travels this far: generating then
submitting a kenobi graph.

It is the job of the kenobi graph to construct the publisher objects and call their methods once the graph is loaded and
 executed on the farm. This is similar to a three-act stage play:

```
UI -> Kenobi Graph -> Publish
```

where each act can be tested in isolation using the output from the previous act.

#### "kenobi driver" ####

The focus here is to test the last act (Publish) so the setup() logic in the tests need some pre-generated kenobi graph
files. We can usually get them by manually running the application.

Once we've collected a number of kenobi graphs with each corresponding to a test subject (publishing motion,
  publishing camera, publishing previs geometry etc...), writing the tests is just a matter of utilizing the kenobi API.

Because this particular testing technique is centered around kenobi graphs, which "drive" the execution of the business
logic, it is titled Kenobi driver.

The conclusion is that if you can identify the boundary between various sub-systems,
try to **capture the data that travels across the boundary** hencing allowing you to reuse or recreate such data for
testing purpose.
